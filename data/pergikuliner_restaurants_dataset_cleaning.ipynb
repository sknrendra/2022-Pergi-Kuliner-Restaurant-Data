{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from bs4 import BeautifulSoup\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_before = pd.read_csv('~/Documents/GitHub/2022-Pergi-Kuliner-Restaurant-Data/pergikuliner_restaurants_dataset.csv')\n",
    "df_before.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cleared\n",
    "#col 1, 2, 3, 9, 10 and 12 do not have nans.\n",
    "#col 4: fixed branch data does not exist and the columns got filled with other information. \n",
    "#col 5: cleared. look precise enough, removed the quotation marks.\n",
    "#col 11: fixed 57 phone number nans. some of these are just not scraped.\n",
    "#col 13: fixed bot does not scrape rating when it goes below 4.\n",
    "#col4: fixed put 'kapasitas kursi' in the right cols.\n",
    "\n",
    "#awaiting script rerun\n",
    "#col 6: promotional texts does not get scraped correctly. added str() to get_text() method but the script needs to be rerun.\n",
    "#col 8: schedule does not get scraped correctly. added str() to get_text() method but the script needs to be rerun."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chromeOptions = Options()\n",
    "chromeOptions.headless = True\n",
    "browserDriver = webdriver.Chrome(ChromeDriverManager().install(), options=chromeOptions)\n",
    "\n",
    "bucket = []\n",
    "for u, n in zip(df_before['3'], range(len(df_before))):\n",
    "    print(f'retrieving from {u}, {n}th website')\n",
    "    browserDriver.get(u)\n",
    "    soup = BeautifulSoup(browserDriver.page_source)\n",
    "    \n",
    "    results = soup.find(attrs={'itemprop': 'openingHours'}).get_text() if not None else None\n",
    "    results = soup.find(attrs={'class': 'promotional-text'}).get_text() if not None else None\n",
    "    results = soup.find(attrs ={'href' : re.compile('tel:')}).get_text()\n",
    "    results = ','.join([i.getText().strip() for i in soup.find_all(class_='rate-box-bottom')])\n",
    "\n",
    "    bucket.append(pd.Series(subbucket))\n",
    "\n",
    "browserDriver.close()\n",
    "\n",
    "df = pd.concat(bucket)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chromeOptions = Options()\n",
    "chromeOptions.headless = True\n",
    "browserDriver = webdriver.Chrome(executable_path=\"/usr/local/bin/chromedriver\", options=chromeOptions)\n",
    "\n",
    "results = []\n",
    "for u, n in zip(df_before['3'], range(len(df_before))):\n",
    "    print(f'retrieving from {u}, {n}th website, {round(n/1500*100,1)}%                              ', end=\"\\r\")\n",
    "    browserDriver.get(u)\n",
    "    soup = BeautifulSoup(browserDriver.page_source)\n",
    "\n",
    "    itemdict = {'itemprop': 'openingHours', 'class': 'promotional-text', 'href' : re.compile('tel:')}\n",
    "    resultbucket = []\n",
    "    for k, v in itemdict.items():\n",
    "        tag = soup.find(attrs={k: v})\n",
    "        if tag is not None:\n",
    "            result = tag.get_text() \n",
    "        else:\n",
    "            result = None\n",
    "        resultbucket.append(result)\n",
    "        \n",
    "    ratings = ','.join([i.getText().strip() for i in soup.find_all(class_='rate-box-bottom')])\n",
    "    resultbucket.append(ratings)\n",
    "\n",
    "results.append(resultbucket)\n",
    "\n",
    "browserDriver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fixed put 'kapasitas kursi' in the right cols.\n",
    "pd.set_option('max_colwidth', 120)\n",
    "df['14'] = df_before.loc[df_before['4'].str.contains('Kapasitas'), '4']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fixed branch data does not exist and the columns got filled with other information\n",
    "df['4'] = df['4'].str.contains('Cabang: ya')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fixed 57 phone number nans. some of these are just not scraped.\n",
    "def retrieve_phone(ulink):\n",
    "    browser = webdriver.Chrome(executable_path=\"/usr/local/bin/chromedriver\")\n",
    "    browser.get(ulink)\n",
    "    soup = BeautifulSoup(browser.page_source)\n",
    "    phone = str(soup.find(href=re.compile('tel:')).get_text())\n",
    "    browser.close()\n",
    "    return phone\n",
    "\n",
    "missing_phone_url = df.loc[df['11'].isna(), '3']\n",
    "\n",
    "for nums in missing_phone_url.index:\n",
    "    missing_phone_url[nums] = retrieve_phone(missing_phone_url[nums])\n",
    "\n",
    "df.loc[df['11'].isna(), '11'] = missing_phone_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fixed bot does not scrape rating when it goes below 4.\n",
    "def retrieve_ratings(ulink):\n",
    "    browser = webdriver.Chrome(executable_path=\"/usr/local/bin/chromedriver\")\n",
    "    browser.get(ulink)\n",
    "    soup = BeautifulSoup(browser.page_source)\n",
    "    phone = ','.join([i.getText().strip() for i in soup.find_all(class_='rate-box-bottom')])\n",
    "    browser.close()\n",
    "    return phone\n",
    "\n",
    "missing_ratings = df.loc[df.iloc[:,13].str.len() < 19, '13']\n",
    "\n",
    "for idx in missing_ratings.index:\n",
    "    missing_ratings[idx] = retrieve_ratings(df.loc[idx, '3'])\n",
    "\n",
    "df.loc[df.iloc[:,13].str.len() < 19, '13'] = missing_ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checks\n",
    "df['13'].str.len().value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checks\n",
    "missing_phone_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checks\n",
    "df['5'].str.replace('\"', '').str.split(',', expand=True).stack().value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['4'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fixed\n",
    "df['5'] = df['5'].str.replace('\"', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "['Name', 'culinary_type', 'address', 'url', '']"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b9cc3ea14771d1f25959520c6cee93dc2b2fa57b75d1fe2bce77bd224877f6be"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 ('dsenv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
